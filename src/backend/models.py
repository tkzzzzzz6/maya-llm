#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹åŠ è½½å’Œç®¡ç†
Model Loading and Management
"""

import gradio as gr
from funasr import AutoModel
from transformers import AutoModelForCausalLM, AutoTokenizer
from modelscope.pipelines import pipeline
from modelscope import snapshot_download

from .config import MODEL_CONFIG, VOICEPRINT_DIR
from .memory import ChatMemory
import os

class MayaModels:
    """æ¨¡å‹ç®¡ç†ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨"""
        self.models_loaded = False
        self.asr_model = None
        self.llm_model = None
        self.llm_tokenizer = None
        self.sv_pipeline = None
        self.memory = ChatMemory(max_length=512)
        
        # ç¡®ä¿å£°çº¹ç›®å½•å­˜åœ¨
        os.makedirs(VOICEPRINT_DIR, exist_ok=True)
        
    def load_models(self, progress=gr.Progress()):
        """
        åŠ è½½æ‰€æœ‰æ¨¡å‹ï¼ˆç”Ÿæˆå™¨å‡½æ•°ï¼Œç”¨äºæµå¼æ›´æ–°çŠ¶æ€ï¼‰
        
        Yields:
            str: åŠ è½½çŠ¶æ€ä¿¡æ¯
        """
        if self.models_loaded:
            yield "âœ… æ¨¡å‹å·²åŠ è½½å®Œæˆ"
            return
        
        try:
            # 1. åŠ è½½ SenseVoice ASR æ¨¡å‹
            yield "ğŸ“¥ æ­£åœ¨åŠ è½½è¯­éŸ³è¯†åˆ«æ¨¡å‹ (SenseVoice)..."
            if progress:
                progress(0.2)
            
            self.asr_model = AutoModel(
                model=MODEL_CONFIG["asr"]["model_id"],
                trust_remote_code=MODEL_CONFIG["asr"]["trust_remote_code"]
            )
            
            # 2. åŠ è½½ CAM++ å£°çº¹è¯†åˆ«æ¨¡å‹
            yield "ğŸ“¥ æ­£åœ¨åŠ è½½å£°çº¹è¯†åˆ«æ¨¡å‹ (CAM++)..."
            if progress:
                progress(0.4)
            
            self.sv_pipeline = pipeline(
                task=MODEL_CONFIG["sv"]["task"],
                model=MODEL_CONFIG["sv"]["model_id"],
                model_revision=MODEL_CONFIG["sv"]["model_revision"]
            )
            
            # 3. åŠ è½½ Qwen2.5 å¤§è¯­è¨€æ¨¡å‹
            yield "ğŸ“¥ æ­£åœ¨åŠ è½½å¤§è¯­è¨€æ¨¡å‹ (Qwen2.5-1.5B)..."
            if progress:
                progress(0.6)
            
            qwen_local_dir = snapshot_download(
                model_id=MODEL_CONFIG["llm"]["model_id"]
            )
            
            yield "ğŸ”„ åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹..."
            if progress:
                progress(0.8)
            
            self.llm_model = AutoModelForCausalLM.from_pretrained(
                qwen_local_dir,
                torch_dtype=MODEL_CONFIG["llm"]["torch_dtype"],
                device_map=MODEL_CONFIG["llm"]["device_map"],
                trust_remote_code=MODEL_CONFIG["llm"]["trust_remote_code"]
            )
            
            self.llm_tokenizer = AutoTokenizer.from_pretrained(
                qwen_local_dir,
                trust_remote_code=MODEL_CONFIG["llm"]["trust_remote_code"]
            )
            
            if progress:
                progress(1.0)
            
            self.models_loaded = True
            yield "âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½æˆåŠŸï¼å¯ä»¥å¼€å§‹å¯¹è¯äº†"
            
        except Exception as e:
            yield f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
    
    def is_loaded(self):
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
        return self.models_loaded
    
    def clear_memory(self):
        """æ¸…ç©ºå¯¹è¯è®°å¿†"""
        self.memory.clear()

# åˆ›å»ºå…¨å±€æ¨¡å‹å®ä¾‹
maya_models = MayaModels()

