import cv2
import pyaudio
import wave
import threading
import numpy as np
import time
from queue import Queue
import webrtcvad
import os
import threading
from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from transformers import AutoModelForCausalLM, AutoTokenizer
from qwen_vl_utils import process_vision_info
import torch
from funasr import AutoModel
import pygame
# import edge_tts
from kokoro import KPipeline
import asyncio
from time import sleep
import langid
from langdetect import detect
from IPython.display import display, Audio
import soundfile as sf

# kokoro è¯­éŸ³åˆæˆ
# ğŸ‡ªğŸ‡¸ 'e' => Spanish es
# ğŸ‡«ğŸ‡· 'f' => French fr-fr
# ğŸ‡®ğŸ‡³ 'h' => Hindi hi
# ğŸ‡®ğŸ‡¹ 'i' => Italian it
# ğŸ‡§ğŸ‡· 'p' => Brazilian Portuguese pt-br
# ğŸ‡ºğŸ‡¸ 'a' => American English, ğŸ‡¬ğŸ‡§ 'b' => British English
# ğŸ‡¯ğŸ‡µ 'j' => Japanese: pip install misaki[ja]
# ğŸ‡¨ğŸ‡³ 'z' => Mandarin Chinese: pip install misaki[zh]
root_voice = r'E:\2_PYTHON\Project\TTS\Kokoro-82M\voices'

def tts_kokoro(text, outpath, lid='z', voice_glo='zm_yunjian'):
    global root_voice
    pipeline = KPipeline(lang_code=lid)
    voice_tensor = torch.load(os.path.join(root_voice, voice_glo+'.pt'), weights_only=True)
    generator = pipeline(
        text, voice=voice_tensor,
        speed=1, split_pattern=r'\n+'
    )

    for i, (gs, ps, audio) in enumerate(generator):
        # display(Audio(data=audio, rate=24000, autoplay=i==0))
        sf.write(f'{outpath}', audio, 24000) # save each audio file

# --- é…ç½®huggingFaceå›½å†…é•œåƒ ---
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# å‚æ•°è®¾ç½®
AUDIO_RATE = 16000        # éŸ³é¢‘é‡‡æ ·ç‡
AUDIO_CHANNELS = 1        # å•å£°é“
CHUNK = 1024              # éŸ³é¢‘å—å¤§å°
VAD_MODE = 3              # VAD æ¨¡å¼ (0-3, æ•°å­—è¶Šå¤§è¶Šæ•æ„Ÿ)
OUTPUT_DIR = "./output"   # è¾“å‡ºç›®å½•
NO_SPEECH_THRESHOLD = 1   # æ— æ•ˆè¯­éŸ³é˜ˆå€¼ï¼Œå•ä½ï¼šç§’
folder_path = "./Test_QWen2_VL/"
audio_file_count = 0

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(folder_path, exist_ok=True)

# é˜Ÿåˆ—ç”¨äºéŸ³é¢‘å’Œè§†é¢‘åŒæ­¥ç¼“å­˜
audio_queue = Queue()
video_queue = Queue()

# å…¨å±€å˜é‡
last_active_time = time.time()
recording_active = True
segments_to_save = []
saved_intervals = []
last_vad_end_time = 0  # ä¸Šæ¬¡ä¿å­˜çš„ VAD æœ‰æ•ˆæ®µç»“æŸæ—¶é—´

# åˆå§‹åŒ– WebRTC VAD
vad = webrtcvad.Vad()
vad.set_mode(VAD_MODE)

# éŸ³é¢‘å½•åˆ¶çº¿ç¨‹
def audio_recorder():
    global audio_queue, recording_active, last_active_time, segments_to_save, last_vad_end_time
    
    p = pyaudio.PyAudio()
    stream = p.open(format=pyaudio.paInt16,
                    channels=AUDIO_CHANNELS,
                    rate=AUDIO_RATE,
                    input=True,
                    frames_per_buffer=CHUNK)
    
    audio_buffer = []
    print("éŸ³é¢‘å½•åˆ¶å·²å¼€å§‹")
    
    while recording_active:
        data = stream.read(CHUNK)
        audio_buffer.append(data)
        
        # æ¯ 0.5 ç§’æ£€æµ‹ä¸€æ¬¡ VAD
        if len(audio_buffer) * CHUNK / AUDIO_RATE >= 0.5:
            # æ‹¼æ¥éŸ³é¢‘æ•°æ®å¹¶æ£€æµ‹ VAD
            raw_audio = b''.join(audio_buffer)
            vad_result = check_vad_activity(raw_audio)
            
            if vad_result:
                print("æ£€æµ‹åˆ°è¯­éŸ³æ´»åŠ¨")
                last_active_time = time.time()
                segments_to_save.append((raw_audio, time.time()))
            else:
                print("é™éŸ³ä¸­...")
            
            audio_buffer = []  # æ¸…ç©ºç¼“å†²åŒº
        
        # æ£€æŸ¥æ— æ•ˆè¯­éŸ³æ—¶é—´
        if time.time() - last_active_time > NO_SPEECH_THRESHOLD:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜
            if segments_to_save and segments_to_save[-1][1] > last_vad_end_time:
                save_audio_video()
                last_active_time = time.time()
            else:
                pass
                # print("æ— æ–°å¢è¯­éŸ³æ®µï¼Œè·³è¿‡ä¿å­˜")
    
    stream.stop_stream()
    stream.close()
    p.terminate()

# è§†é¢‘å½•åˆ¶çº¿ç¨‹
def video_recorder():
    global video_queue, recording_active
    
    cap = cv2.VideoCapture(0)  # ä½¿ç”¨é»˜è®¤æ‘„åƒå¤´
    print("è§†é¢‘å½•åˆ¶å·²å¼€å§‹")
    
    while recording_active:
        ret, frame = cap.read()
        if ret:
            video_queue.put((frame, time.time()))
            
            # å®æ—¶æ˜¾ç¤ºæ‘„åƒå¤´ç”»é¢
            cv2.imshow("Real Camera", frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):  # æŒ‰ Q é”®é€€å‡º
                break
        else:
            print("æ— æ³•è·å–æ‘„åƒå¤´ç”»é¢")
    
    cap.release()
    cv2.destroyAllWindows()

# æ£€æµ‹ VAD æ´»åŠ¨
def check_vad_activity(audio_data):
    # å°†éŸ³é¢‘æ•°æ®åˆ†å—æ£€æµ‹
    num, rate = 0, 0.4
    step = int(AUDIO_RATE * 0.02)  # 20ms å—å¤§å°
    flag_rate = round(rate * len(audio_data) // step)

    for i in range(0, len(audio_data), step):
        chunk = audio_data[i:i + step]
        if len(chunk) == step:
            if vad.is_speech(chunk, sample_rate=AUDIO_RATE):
                num += 1

    if num > flag_rate:
        return True
    return False

# ä¿å­˜éŸ³é¢‘å’Œè§†é¢‘
def save_audio_video():
    pygame.mixer.init()

    global segments_to_save, video_queue, last_vad_end_time, saved_intervals

    # å…¨å±€å˜é‡ï¼Œç”¨äºä¿å­˜éŸ³é¢‘æ–‡ä»¶åè®¡æ•°
    global audio_file_count
    audio_file_count += 1
    audio_output_path = f"{OUTPUT_DIR}/audio_{audio_file_count}.wav"
    # audio_output_path = f"{OUTPUT_DIR}/audio_0.wav"

    if not segments_to_save:
        return
    
    # åœæ­¢å½“å‰æ’­æ”¾çš„éŸ³é¢‘
    if pygame.mixer.music.get_busy():
        pygame.mixer.music.stop()
        print("æ£€æµ‹åˆ°æ–°çš„æœ‰æ•ˆéŸ³ï¼Œå·²åœæ­¢å½“å‰éŸ³é¢‘æ’­æ”¾")
        
    # è·å–æœ‰æ•ˆæ®µçš„æ—¶é—´èŒƒå›´
    start_time = segments_to_save[0][1]
    end_time = segments_to_save[-1][1]
    
    # æ£€æŸ¥æ˜¯å¦ä¸ä¹‹å‰çš„ç‰‡æ®µé‡å 
    if saved_intervals and saved_intervals[-1][1] >= start_time:
        print("å½“å‰ç‰‡æ®µä¸ä¹‹å‰ç‰‡æ®µé‡å ï¼Œè·³è¿‡ä¿å­˜")
        segments_to_save.clear()
        return
    
    # ä¿å­˜éŸ³é¢‘
    audio_frames = [seg[0] for seg in segments_to_save]
    
    wf = wave.open(audio_output_path, 'wb')
    wf.setnchannels(AUDIO_CHANNELS)
    wf.setsampwidth(2)  # 16-bit PCM
    wf.setframerate(AUDIO_RATE)
    wf.writeframes(b''.join(audio_frames))
    wf.close()
    print(f"éŸ³é¢‘ä¿å­˜è‡³ {audio_output_path}")
    
    # Inference()
    # ä½¿ç”¨çº¿ç¨‹æ‰§è¡Œæ¨ç†
    inference_thread = threading.Thread(target=Inference, args=(audio_output_path,))
    inference_thread.start()
        
    # è®°å½•ä¿å­˜çš„åŒºé—´
    saved_intervals.append((start_time, end_time))
    
    # æ¸…ç©ºç¼“å†²åŒº
    segments_to_save.clear()

# --- æ’­æ”¾éŸ³é¢‘ -
def play_audio(file_path):
    try:
        pygame.mixer.init()
        pygame.mixer.music.load(file_path)
        pygame.mixer.music.play()
        while pygame.mixer.music.get_busy():
            time.sleep(1)  # ç­‰å¾…éŸ³é¢‘æ’­æ”¾ç»“æŸ
        print("æ’­æ”¾å®Œæˆï¼")
    except Exception as e:
        print(f"æ’­æ”¾å¤±è´¥: {e}")
    finally:
        pygame.mixer.quit()

# async def amain(TEXT, VOICE, OUTPUT_FILE) -> None:
#     """Main function"""
#     communicate = edge_tts.Communicate(TEXT, VOICE)
#     await communicate.save(OUTPUT_FILE)


# -------- SenceVoice è¯­éŸ³è¯†åˆ« --æ¨¡å‹åŠ è½½-----
model_dir = r"E:\2_PYTHON\Project\GPT\QWen\pretrained_models\SenseVoiceSmall"
model_senceVoice = AutoModel( model=model_dir, trust_remote_code=True, )

# --- QWen2.5å¤§è¯­è¨€æ¨¡å‹ ---
# model_name = r"E:\2_PYTHON\Project\GPT\QWen\Qwen2.5-0.5B-Instruct"
model_name = r"E:\2_PYTHON\Project\GPT\QWen\Qwen2.5-1.5B-Instruct"
# model_name = r'E:\2_PYTHON\Project\GPT\QWen\Qwen2.5-7B-Instruct-GPTQ-Int4'
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

def Inference(TEMP_AUDIO_FILE=f"{OUTPUT_DIR}/audio_0.wav"):
    # -------- SenceVoice æ¨ç† ---------
    input_file = (TEMP_AUDIO_FILE)
    res = model_senceVoice.generate(
        input=input_file,
        cache={},
        language="auto", # "zn", "en", "yue", "ja", "ko", "nospeech"
        use_itn=False,
    )
    # prompt = res[0]['text'].split(">")[-1]
    prompt = res[0]['text'].split(">")[-1] + "ï¼Œå›ç­”ç®€çŸ­ä¸€äº›ï¼Œä¿æŒ50å­—ä»¥å†…ï¼"
    print("ASR OUT:", prompt)
    # ---------SenceVoice --end----------
    # -------- æ¨¡å‹æ¨ç†é˜¶æ®µï¼Œå°†è¯­éŸ³è¯†åˆ«ç»“æœä½œä¸ºå¤§æ¨¡å‹Prompt ------
    messages = [
        {"role": "system", "content": "ä½ å«åƒé—®ï¼Œæ˜¯ä¸€ä¸ª18å²çš„å¥³å¤§å­¦ç”Ÿï¼Œæ€§æ ¼æ´»æ³¼å¼€æœ—ï¼Œè¯´è¯ä¿çš®"},
        {"role": "user", "content": prompt},
    ]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=512,
    )
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    print("answer", output_text)

    # è¾“å…¥æ–‡æœ¬
    text = output_text
    # è¯­ç§è¯†åˆ« -- langid
    language, confidence = langid.classify(text)
    # è¯­ç§è¯†åˆ« -- langdetect 
    # language = detect(text).split("-")[0]

    # ğŸ‡ªğŸ‡¸ 'e' => Spanish es
    # ğŸ‡«ğŸ‡· 'f' => French fr-fr
    # ğŸ‡®ğŸ‡³ 'h' => Hindi hi
    # ğŸ‡®ğŸ‡¹ 'i' => Italian it
    # ğŸ‡§ğŸ‡· 'p' => Brazilian Portuguese pt-br
    # ğŸ‡ºğŸ‡¸ 'a' => American English, ğŸ‡¬ğŸ‡§ 'b' => British English
    # ğŸ‡¯ğŸ‡µ 'j' => Japanese: pip install misaki[ja]
    # ğŸ‡¨ğŸ‡³ 'z' => Mandarin Chinese: pip install misaki[zh]

    language_speaker = {
        "ja" : "j",            # ok
        "fr" : "f",            # ok
        "es" : "e",            # ok
        "zh" : "z",            # ok
        "en" : "a",            # ok
    }

    language_spk = {
        "j" : "jf_nezumi",            # ok
        "f" : "ff_siwis",            # ok
        "e" : "em_santa",            # ok
        "z" : "zm_yunyang",            # ok
        "a" : "af_heart",            # ok
    }

    if language not in language_speaker.keys():
        used_speaker = "z"
    else:
        used_speaker = language_speaker[language]
        print("æ£€æµ‹åˆ°è¯­ç§ï¼š", language, "ä½¿ç”¨éŸ³è‰²ï¼š", language_speaker[language])

    global audio_file_count
    outpath = os.path.join(folder_path,f"sft_{audio_file_count}.wav")
    tts_kokoro(text, outpath, lid=used_speaker, voice_glo=language_spk[used_speaker])
    play_audio(f'{folder_path}/sft_{audio_file_count}.wav')

# ä¸»å‡½æ•°
if __name__ == "__main__":

    try:
        # å¯åŠ¨éŸ³è§†é¢‘å½•åˆ¶çº¿ç¨‹
        audio_thread = threading.Thread(target=audio_recorder)
        # video_thread = threading.Thread(target=video_recorder)
        audio_thread.start()
        # video_thread.start()
        
        print("æŒ‰ Ctrl+C åœæ­¢å½•åˆ¶")
        while True:
            time.sleep(1)
    
    except KeyboardInterrupt:
        print("å½•åˆ¶åœæ­¢ä¸­...")
        recording_active = False
        audio_thread.join()
        # video_thread.join()
        print("å½•åˆ¶å·²åœæ­¢")
