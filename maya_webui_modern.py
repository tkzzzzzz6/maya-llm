#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éº»é¸­è¯­éŸ³åŠ©æ‰‹ - ç°ä»£åŒ– Web UI (ChatGPT/Claude é£æ ¼)
Maya Voice Assistant - Modern ChatGPT-style Interface

ç‰¹æ€§ï¼š
- ğŸ¨ ç°ä»£åŒ–èŠå¤©ç•Œé¢ï¼ˆå¯¹è¯æ°”æ³¡ï¼‰
- ğŸŒ“ æ·±è‰²/æµ…è‰²ä¸»é¢˜
- ğŸ“± å“åº”å¼å¸ƒå±€
- ğŸ’¬ æµå¼è¾“å‡º
- ğŸ™ï¸ è¯­éŸ³äº¤äº’
"""

import gradio as gr
import numpy as np
import torch
import wave
import os
import tempfile
import asyncio
from datetime import datetime
from funasr import AutoModel
from transformers import AutoModelForCausalLM, AutoTokenizer
from modelscope.pipelines import pipeline
from modelscope import snapshot_download
import edge_tts
import pygame
import time
import re
from pypinyin import pinyin, Style
import langid
import json

# ========== é…ç½® ==========
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# å…¨å±€å˜é‡
audio_file_count = 0
chat_history = []

# ========== å·¥å…·å‡½æ•° ==========

def extract_chinese_and_convert_to_pinyin(input_string):
    """æå–æ±‰å­—å¹¶è½¬æ¢ä¸ºæ‹¼éŸ³"""
    chinese_characters = re.findall(r'[\u4e00-\u9fa5]', input_string)
    chinese_text = ''.join(chinese_characters)
    pinyin_result = pinyin(chinese_text, style=Style.NORMAL)
    pinyin_text = ' '.join([item[0] for item in pinyin_result])
    return pinyin_text

def is_folder_empty(folder_path):
    """æ£€æµ‹æ–‡ä»¶å¤¹æ˜¯å¦ä¸ºç©º"""
    if not os.path.exists(folder_path):
        return True
    entries = os.listdir(folder_path)
    for entry in entries:
        full_path = os.path.join(folder_path, entry)
        if os.path.isfile(full_path):
            return False
    return True

async def text_to_speech(text, voice="zh-CN-XiaoyiNeural"):
    """å¼‚æ­¥æ–‡å­—è½¬è¯­éŸ³"""
    output_file = tempfile.mktemp(suffix=".mp3")
    communicate = edge_tts.Communicate(text, voice)
    await communicate.save(output_file)
    return output_file

# ========== å¯¹è¯è®°å¿†ç±» ==========

class ChatMemory:
    def __init__(self, max_length=2048):
        self.history = []
        self.max_length = max_length

    def add_to_history(self, user_input, model_response):
        self.history.append(f"User: {user_input}")
        self.history.append(f"Assistant: {model_response}")

    def get_context(self):
        context = "\n".join(self.history)
        if len(context) > self.max_length:
            context = context[-self.max_length:]
        return context
    
    def clear(self):
        self.history = []

# ========== æ¨¡å‹ç®¡ç†ç±» ==========

class MayaModels:
    def __init__(self):
        self.models_loaded = False
        self.asr_model = None
        self.llm_model = None
        self.llm_tokenizer = None
        self.sv_pipeline = None
        self.memory = ChatMemory(max_length=512)
        self.sv_enroll_dir = './SpeakerVerification_DIR/enroll_wav/'
        os.makedirs(self.sv_enroll_dir, exist_ok=True)
        os.makedirs('./output', exist_ok=True)
        
    def load_models(self, progress=gr.Progress()):
        """åŠ è½½æ‰€æœ‰æ¨¡å‹"""
        if self.models_loaded:
            yield "âœ… æ¨¡å‹å·²åŠ è½½å®Œæˆ"
            return
        
        try:
            yield "ğŸ“¥ æ­£åœ¨åŠ è½½è¯­éŸ³è¯†åˆ«æ¨¡å‹ (SenseVoice)..."
            progress(0.2)
            self.asr_model = AutoModel(
                model="iic/SenseVoiceSmall",
                trust_remote_code=True
            )
            
            yield "ğŸ“¥ æ­£åœ¨åŠ è½½å£°çº¹è¯†åˆ«æ¨¡å‹ (CAM++)..."
            progress(0.4)
            self.sv_pipeline = pipeline(
                task='speaker-verification',
                model='damo/speech_campplus_sv_zh-cn_16k-common',
                model_revision='v1.0.0'
            )
            
            yield "ğŸ“¥ æ­£åœ¨åŠ è½½å¤§è¯­è¨€æ¨¡å‹ (Qwen2.5-1.5B)..."
            progress(0.6)
            model_id = "qwen/Qwen2.5-1.5B-Instruct"
            qwen_local_dir = snapshot_download(model_id=model_id)
            
            yield "ğŸ”„ åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹..."
            progress(0.8)
            self.llm_model = AutoModelForCausalLM.from_pretrained(
                qwen_local_dir,
                torch_dtype="auto",
                device_map="auto",
                trust_remote_code=True
            )
            self.llm_tokenizer = AutoTokenizer.from_pretrained(
                qwen_local_dir,
                trust_remote_code=True
            )
            
            progress(1.0)
            self.models_loaded = True
            yield "âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½æˆåŠŸï¼å¯ä»¥å¼€å§‹å¯¹è¯äº†"
            
        except Exception as e:
            yield f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"

# åˆ›å»ºå…¨å±€æ¨¡å‹å®ä¾‹
maya_models = MayaModels()

# ========== æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ==========

def register_voiceprint(audio_file):
    """å£°çº¹æ³¨å†Œ"""
    if audio_file is None:
        return "âŒ è¯·å…ˆå½•åˆ¶éŸ³é¢‘"
    
    try:
        # æ£€æŸ¥éŸ³é¢‘é•¿åº¦
        with wave.open(audio_file, 'rb') as wf:
            frames = wf.getnframes()
            rate = wf.getframerate()
            duration = frames / float(rate)
        
        if duration < 3:
            return f"âŒ éŸ³é¢‘æ—¶é•¿ä»… {duration:.1f} ç§’ï¼Œéœ€è¦è‡³å°‘ 3 ç§’"
        
        # ä¿å­˜å£°çº¹æ–‡ä»¶
        enroll_path = os.path.join(maya_models.sv_enroll_dir, "enroll_0.wav")
        import shutil
        shutil.copy(audio_file, enroll_path)
        
        return f"âœ… å£°çº¹æ³¨å†ŒæˆåŠŸï¼éŸ³é¢‘æ—¶é•¿: {duration:.1f} ç§’"
        
    except Exception as e:
        return f"âŒ å£°çº¹æ³¨å†Œå¤±è´¥: {str(e)}"

def speech_to_text(audio_file):
    """è¯­éŸ³è¯†åˆ«"""
    if audio_file is None:
        return ""
    
    if not maya_models.models_loaded:
        return ""
    
    try:
        res = maya_models.asr_model.generate(
            input=audio_file,
            cache={},
            language="auto",
            use_itn=False,
        )
        
        text = res[0]['text'].split(">")[-1]
        return text
        
    except Exception as e:
        return f"è¯†åˆ«å¤±è´¥: {str(e)}"

def verify_speaker(audio_file, threshold=0.35):
    """å£°çº¹éªŒè¯"""
    if audio_file is None:
        return False, 0.0
    
    if not maya_models.models_loaded:
        return False, 0.0
    
    enroll_file = os.path.join(maya_models.sv_enroll_dir, "enroll_0.wav")
    if not os.path.exists(enroll_file):
        return False, 0.0
    
    try:
        sv_score = maya_models.sv_pipeline(
            [enroll_file, audio_file],
            thr=threshold
        )
        
        score = sv_score.get('score', 0.0)
        result = sv_score.get('text', 'no')
        
        return result == "yes", score
            
    except Exception as e:
        return False, 0.0

def check_wake_word(text, wake_word="ç«™èµ·æ¥"):
    """æ£€æŸ¥å”¤é†’è¯"""
    text_pinyin = extract_chinese_and_convert_to_pinyin(text)
    wake_word_pinyin = extract_chinese_and_convert_to_pinyin(wake_word)
    
    wake_word_pinyin = wake_word_pinyin.replace(" ", "")
    text_pinyin = text_pinyin.replace(" ", "")
    
    return wake_word_pinyin in text_pinyin

def chat_respond(message, history, audio_input, settings):
    """
    ä¸»å¯¹è¯å‡½æ•° - æµå¼è¾“å‡º
    settings æ ¼å¼: {
        "enable_kws": bool,
        "wake_word": str,
        "enable_sv": bool,
        "sv_threshold": float,
        "enable_tts": bool,
        "system_prompt": str
    }
    """
    global audio_file_count
    
    if not maya_models.models_loaded:
        yield history + [("ç³»ç»Ÿ", "âŒ è¯·å…ˆåœ¨è®¾ç½®ä¸­åŠ è½½æ¨¡å‹")], None
        return
    
    # 1. å¤„ç†è¾“å…¥ï¼ˆæ–‡å­—æˆ–è¯­éŸ³ï¼‰
    user_text = message
    if audio_input is not None:
        asr_text = speech_to_text(audio_input)
        if asr_text:
            user_text = asr_text
    
    if not user_text or user_text.strip() == "":
        yield history, None
        return
    
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    history = history + [(user_text, None)]
    yield history, None
    
    # 2. å…³é”®è¯å”¤é†’æ£€æµ‹
    if settings.get("enable_kws", False):
        wake_word = settings.get("wake_word", "ç«™èµ·æ¥")
        if not check_wake_word(user_text, wake_word):
            error_msg = f"âš ï¸ æœªæ£€æµ‹åˆ°å”¤é†’è¯ã€Œ{wake_word}ã€"
            history[-1] = (user_text, error_msg)
            yield history, None
            return
    
    # 3. å£°çº¹éªŒè¯
    if settings.get("enable_sv", False) and audio_input is not None:
        sv_threshold = settings.get("sv_threshold", 0.35)
        sv_pass, sv_score = verify_speaker(audio_input, sv_threshold)
        if not sv_pass:
            error_msg = f"âš ï¸ å£°çº¹éªŒè¯å¤±è´¥ (ç›¸ä¼¼åº¦: {sv_score:.2f})"
            history[-1] = (user_text, error_msg)
            yield history, None
            return
    
    # 4. å¤§è¯­è¨€æ¨¡å‹æ¨ç†
    try:
        context = maya_models.memory.get_context()
        prompt_with_context = f"{context}\nUser: {user_text}\n" if context else user_text
        
        system_prompt = settings.get("system_prompt", "ä½ å«å°åƒï¼Œæ˜¯ä¸€ä¸ª18å²çš„å¥³å¤§å­¦ç”Ÿï¼Œæ€§æ ¼æ´»æ³¼å¼€æœ—ï¼Œè¯´è¯ä¿çš®ç®€æ´ï¼Œå›ç­”é—®é¢˜ä¸ä¼šè¶…è¿‡50å­—ã€‚")
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt_with_context},
        ]
        
        text = maya_models.llm_tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        
        model_inputs = maya_models.llm_tokenizer([text], return_tensors="pt").to(
            maya_models.llm_model.device
        )
        
        generated_ids = maya_models.llm_model.generate(
            **model_inputs,
            max_new_tokens=512,
        )
        
        generated_ids = [
            output_ids[len(input_ids):]
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        output_text = maya_models.llm_tokenizer.batch_decode(
            generated_ids, skip_special_tokens=True
        )[0]
        
        # æµå¼æ˜¾ç¤ºå›å¤
        history[-1] = (user_text, "")
        for i in range(0, len(output_text), 2):
            history[-1] = (user_text, output_text[:i+2])
            yield history, None
            time.sleep(0.02)
        
        history[-1] = (user_text, output_text)
        
        # æ›´æ–°è®°å¿†
        maya_models.memory.add_to_history(user_text, output_text)
        
        # 5. è¯­éŸ³åˆæˆ
        audio_output = None
        if settings.get("enable_tts", True):
            language, _ = langid.classify(output_text)
            
            language_speaker = {
                "ja": "ja-JP-NanamiNeural",
                "fr": "fr-FR-DeniseNeural",
                "es": "ca-ES-JoanaNeural",
                "de": "de-DE-KatjaNeural",
                "zh": "zh-CN-XiaoyiNeural",
                "en": "en-US-AnaNeural",
            }
            
            voice = language_speaker.get(language, "zh-CN-XiaoyiNeural")
            audio_file_count += 1
            audio_output = asyncio.run(text_to_speech(output_text, voice))
        
        yield history, audio_output
        
    except Exception as e:
        error_msg = f"âŒ å‡ºé”™äº†: {str(e)}"
        history[-1] = (user_text, error_msg)
        yield history, None

# ========== ç°ä»£åŒ– Gradio ç•Œé¢ ==========

def create_modern_ui():
    """åˆ›å»ºç°ä»£åŒ–ç•Œé¢"""
    
    # è¶…çº§ç°ä»£åŒ–çš„ CSS
    modern_css = """
    /* å…¨å±€æ ·å¼ */
    :root {
        --primary-color: #10a37f;
        --secondary-color: #19c37d;
        --bg-primary: #ffffff;
        --bg-secondary: #f7f7f8;
        --bg-tertiary: #ececf1;
        --text-primary: #0d0d0d;
        --text-secondary: #676767;
        --border-color: #e5e5e5;
        --shadow: 0 2px 8px rgba(0,0,0,0.1);
        --radius: 12px;
    }
    
    /* æ·±è‰²æ¨¡å¼ */
    .dark {
        --bg-primary: #212121;
        --bg-secondary: #2f2f2f;
        --bg-tertiary: #424242;
        --text-primary: #ececf1;
        --text-secondary: #c5c5d2;
        --border-color: #4e4e4e;
        --shadow: 0 2px 8px rgba(0,0,0,0.3);
    }
    
    /* ä¸»å®¹å™¨ */
    .gradio-container {
        max-width: 1400px !important;
        margin: 0 auto !important;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }
    
    /* é¡¶éƒ¨æ ‡é¢˜æ  */
    .header-bar {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem 2rem;
        border-radius: var(--radius);
        margin-bottom: 2rem;
        box-shadow: var(--shadow);
    }
    
    .header-title {
        color: white;
        font-size: 2rem;
        font-weight: 700;
        margin: 0;
        display: flex;
        align-items: center;
        gap: 0.75rem;
    }
    
    .header-subtitle {
        color: rgba(255,255,255,0.9);
        font-size: 1rem;
        margin-top: 0.5rem;
    }
    
    /* èŠå¤©å®¹å™¨ */
    .chatbot-container {
        background: var(--bg-primary);
        border-radius: var(--radius);
        box-shadow: var(--shadow);
        overflow: hidden;
    }
    
    /* æ¶ˆæ¯æ°”æ³¡æ ·å¼ */
    .message {
        display: flex;
        gap: 1rem;
        padding: 1.5rem;
        animation: fadeIn 0.3s ease-in;
    }
    
    @keyframes fadeIn {
        from { opacity: 0; transform: translateY(10px); }
        to { opacity: 1; transform: translateY(0); }
    }
    
    .user-message {
        background: var(--bg-secondary);
        border-left: 4px solid var(--primary-color);
    }
    
    .bot-message {
        background: var(--bg-primary);
        border-left: 4px solid #8e8ea0;
    }
    
    /* è¾“å…¥åŒºåŸŸ */
    .input-container {
        background: var(--bg-secondary);
        border-radius: var(--radius);
        padding: 1.5rem;
        box-shadow: var(--shadow);
    }
    
    /* æŒ‰é’®æ ·å¼ */
    .primary-btn {
        background: linear-gradient(135deg, var(--primary-color), var(--secondary-color)) !important;
        border: none !important;
        color: white !important;
        font-weight: 600 !important;
        padding: 0.75rem 2rem !important;
        border-radius: 8px !important;
        transition: all 0.3s ease !important;
        box-shadow: 0 4px 12px rgba(16,163,127,0.3) !important;
    }
    
    .primary-btn:hover {
        transform: translateY(-2px) !important;
        box-shadow: 0 6px 20px rgba(16,163,127,0.4) !important;
    }
    
    /* è®¾ç½®é¢æ¿ */
    .settings-panel {
        background: var(--bg-secondary);
        border-radius: var(--radius);
        padding: 1.5rem;
        box-shadow: var(--shadow);
    }
    
    .settings-section {
        margin-bottom: 1.5rem;
        padding-bottom: 1.5rem;
        border-bottom: 1px solid var(--border-color);
    }
    
    .settings-section:last-child {
        border-bottom: none;
        margin-bottom: 0;
        padding-bottom: 0;
    }
    
    /* çŠ¶æ€æŒ‡ç¤ºå™¨ */
    .status-indicator {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-size: 0.875rem;
        font-weight: 500;
    }
    
    .status-success {
        background: #d1fae5;
        color: #065f46;
    }
    
    .status-loading {
        background: #dbeafe;
        color: #1e40af;
    }
    
    .status-error {
        background: #fee2e2;
        color: #991b1b;
    }
    
    /* ç‰¹æ€§å¡ç‰‡ */
    .feature-card {
        background: var(--bg-primary);
        border-radius: var(--radius);
        padding: 1.5rem;
        box-shadow: var(--shadow);
        transition: all 0.3s ease;
        border: 1px solid var(--border-color);
    }
    
    .feature-card:hover {
        transform: translateY(-4px);
        box-shadow: 0 8px 24px rgba(0,0,0,0.15);
    }
    
    .feature-icon {
        font-size: 2rem;
        margin-bottom: 0.75rem;
    }
    
    .feature-title {
        font-size: 1.125rem;
        font-weight: 600;
        color: var(--text-primary);
        margin-bottom: 0.5rem;
    }
    
    .feature-description {
        color: var(--text-secondary);
        font-size: 0.875rem;
        line-height: 1.5;
    }
    
    /* å“åº”å¼ */
    @media (max-width: 768px) {
        .header-title {
            font-size: 1.5rem;
        }
        
        .message {
            padding: 1rem;
        }
    }
    
    /* åŠ è½½åŠ¨ç”» */
    .loading-dots {
        display: inline-flex;
        gap: 4px;
    }
    
    .loading-dots span {
        width: 8px;
        height: 8px;
        border-radius: 50%;
        background: var(--primary-color);
        animation: bounce 1.4s infinite ease-in-out both;
    }
    
    .loading-dots span:nth-child(1) { animation-delay: -0.32s; }
    .loading-dots span:nth-child(2) { animation-delay: -0.16s; }
    
    @keyframes bounce {
        0%, 80%, 100% { transform: scale(0); }
        40% { transform: scale(1); }
    }
    
    /* Gradio ç»„ä»¶è¦†ç›– */
    .gr-button {
        border-radius: 8px !important;
    }
    
    .gr-input, .gr-textarea {
        border-radius: 8px !important;
        border-color: var(--border-color) !important;
    }
    
    .gr-input:focus, .gr-textarea:focus {
        border-color: var(--primary-color) !important;
        box-shadow: 0 0 0 3px rgba(16,163,127,0.1) !important;
    }
    """
    
    with gr.Blocks(css=modern_css, theme=gr.themes.Soft(), title="éº»é¸­è¯­éŸ³åŠ©æ‰‹") as demo:
        
        # éšè—çš„è®¾ç½®çŠ¶æ€
        settings_state = gr.State({
            "enable_kws": False,
            "wake_word": "ç«™èµ·æ¥",
            "enable_sv": False,
            "sv_threshold": 0.35,
            "enable_tts": True,
            "system_prompt": "ä½ å«å°åƒï¼Œæ˜¯ä¸€ä¸ª18å²çš„å¥³å¤§å­¦ç”Ÿï¼Œæ€§æ ¼æ´»æ³¼å¼€æœ—ï¼Œè¯´è¯ä¿çš®ç®€æ´ï¼Œå›ç­”é—®é¢˜ä¸ä¼šè¶…è¿‡50å­—ã€‚"
        })
        
        # æ ‡é¢˜æ 
        gr.HTML("""
        <div class="header-bar">
            <h1 class="header-title">
                <span>ğŸ¦†</span>
                <span>éº»é¸­è¯­éŸ³åŠ©æ‰‹</span>
            </h1>
            <p class="header-subtitle">
                Maya Voice Assistant - åŸºäº SenseVoice + CAM++ + Qwen2.5 + Edge TTS
            </p>
        </div>
        """)
        
        with gr.Row():
            # å·¦ä¾§ - ä¸»èŠå¤©åŒº
            with gr.Column(scale=3):
                # èŠå¤©ç•Œé¢
                chatbot = gr.Chatbot(
                    label="",
                    height=600,
                    show_copy_button=True,
                    bubble_full_width=False,
                    avatar_images=(None, "ğŸ¦†"),
                    elem_classes="chatbot-container"
                )
                
                # è¾“å…¥åŒº
                with gr.Row():
                    with gr.Column(scale=4):
                        msg = gr.Textbox(
                            label="",
                            placeholder="ğŸ’¬ è¾“å…¥æ¶ˆæ¯... (æ”¯æŒæ–‡å­—å’Œè¯­éŸ³)",
                            show_label=False,
                            container=False
                        )
                    with gr.Column(scale=1, min_width=100):
                        audio_input = gr.Audio(
                            source="microphone",
                            type="filepath",
                            label="ğŸ™ï¸",
                            show_label=False
                        )
                
                with gr.Row():
                    send_btn = gr.Button("ğŸ“¤ å‘é€", variant="primary", size="lg", elem_classes="primary-btn")
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", size="lg")
                
                # éŸ³é¢‘æ’­æ”¾
                audio_output = gr.Audio(label="ğŸ”Š è¯­éŸ³å›å¤", autoplay=True, visible=True)
            
            # å³ä¾§ - è®¾ç½®å’ŒåŠŸèƒ½åŒº
            with gr.Column(scale=1):
                with gr.Tabs():
                    # Tab 1: å¿«é€Ÿè®¾ç½®
                    with gr.Tab("âš™ï¸ è®¾ç½®"):
                        gr.Markdown("### ğŸ›ï¸ å¿«é€Ÿè®¾ç½®")
                        
                        with gr.Group():
                            gr.Markdown("#### æ¨¡å‹åŠ è½½")
                            load_btn = gr.Button("ğŸš€ åŠ è½½æ¨¡å‹", variant="primary", size="sm")
                            load_status = gr.Markdown("è¯·å…ˆåŠ è½½æ¨¡å‹")
                        
                        gr.Markdown("---")
                        
                        with gr.Group():
                            gr.Markdown("#### å¯¹è¯è®¾ç½®")
                            enable_kws = gr.Checkbox(label="ğŸ”‘ å…³é”®è¯å”¤é†’", value=False)
                            wake_word = gr.Textbox(
                                label="å”¤é†’è¯",
                                value="ç«™èµ·æ¥",
                                placeholder="è¾“å…¥å”¤é†’è¯"
                            )
                            
                            enable_sv = gr.Checkbox(label="ğŸ‘¤ å£°çº¹éªŒè¯", value=False)
                            sv_threshold = gr.Slider(
                                minimum=0.1,
                                maximum=0.9,
                                value=0.35,
                                step=0.05,
                                label="éªŒè¯é˜ˆå€¼"
                            )
                            
                            enable_tts = gr.Checkbox(label="ğŸ”Š è¯­éŸ³åˆæˆ", value=True)
                        
                        gr.Markdown("---")
                        
                        with gr.Group():
                            gr.Markdown("#### AI äººè®¾")
                            system_prompt = gr.Textbox(
                                label="ç³»ç»Ÿæç¤ºè¯",
                                value="ä½ å«å°åƒï¼Œæ˜¯ä¸€ä¸ª18å²çš„å¥³å¤§å­¦ç”Ÿï¼Œæ€§æ ¼æ´»æ³¼å¼€æœ—ï¼Œè¯´è¯ä¿çš®ç®€æ´ï¼Œå›ç­”é—®é¢˜ä¸ä¼šè¶…è¿‡50å­—ã€‚",
                                lines=4
                            )
                    
                    # Tab 2: å£°çº¹æ³¨å†Œ
                    with gr.Tab("ğŸ‘¤ å£°çº¹"):
                        gr.Markdown("### ğŸ™ï¸ å£°çº¹æ³¨å†Œ")
                        gr.Markdown("å½•åˆ¶ 3-10 ç§’æ¸…æ™°è¯­éŸ³")
                        
                        voiceprint_audio = gr.Audio(
                            source="microphone",
                            type="filepath",
                            label="å½•åˆ¶éŸ³é¢‘"
                        )
                        register_btn = gr.Button("âœ… æ³¨å†Œ", variant="primary")
                        register_status = gr.Markdown("")
                    
                    # Tab 3: å¸®åŠ©
                    with gr.Tab("â“ å¸®åŠ©"):
                        gr.Markdown("""
                        ### ğŸ“– å¿«é€Ÿå¼€å§‹
                        
                        #### 1ï¸âƒ£ åŠ è½½æ¨¡å‹
                        ç‚¹å‡»"è®¾ç½®"æ ‡ç­¾ä¸­çš„"åŠ è½½æ¨¡å‹"
                        
                        #### 2ï¸âƒ£ å¼€å§‹å¯¹è¯
                        - **æ–‡å­—**: ç›´æ¥è¾“å…¥æ¶ˆæ¯
                        - **è¯­éŸ³**: ç‚¹å‡»éº¦å…‹é£å½•éŸ³
                        
                        #### 3ï¸âƒ£ é«˜çº§åŠŸèƒ½
                        - **å”¤é†’è¯**: éœ€è¦è¯´å‡ºç‰¹å®šè¯æ‰å“åº”
                        - **å£°çº¹**: ä»…æ³¨å†Œç”¨æˆ·å¯ç”¨
                        - **äººè®¾**: è‡ªå®šä¹‰AIæ€§æ ¼
                        
                        ---
                        
                        ### ğŸ’¡ ä½¿ç”¨æŠ€å·§
                        
                        **å¿«é€Ÿå¯¹è¯**
                        - å…³é—­å”¤é†’è¯å’Œå£°çº¹éªŒè¯
                        - ç›´æ¥æ‰“å­—å³å¯
                        
                        **è¯­éŸ³åŠ©æ‰‹**
                        - å¼€å¯å”¤é†’è¯
                        - å½•éŸ³è¯´ï¼š"ç«™èµ·æ¥ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
                        
                        **ä¸“å±åŠ©æ‰‹**
                        - æ³¨å†Œå£°çº¹
                        - å¼€å¯å£°çº¹éªŒè¯
                        - åªæœ‰ä½ èƒ½ä½¿ç”¨
                        
                        ---
                        
                        ### ğŸ¨ ä¸»é¢˜
                        æ”¯æŒæµ…è‰²/æ·±è‰²æ¨¡å¼
                        (æµè§ˆå™¨å³ä¸‹è§’åˆ‡æ¢)
                        """)
        
        # åº•éƒ¨åŠŸèƒ½ç‰¹æ€§å±•ç¤º
        gr.Markdown("---")
        gr.Markdown("### âœ¨ æ ¸å¿ƒåŠŸèƒ½")
        
        with gr.Row():
            gr.HTML("""
            <div class="feature-card">
                <div class="feature-icon">ğŸ™ï¸</div>
                <div class="feature-title">è¯­éŸ³è¯†åˆ«</div>
                <div class="feature-description">SenseVoice é«˜ç²¾åº¦ä¸­è‹±æ–‡è¯†åˆ«</div>
            </div>
            """)
            gr.HTML("""
            <div class="feature-card">
                <div class="feature-icon">ğŸ¤–</div>
                <div class="feature-title">æ™ºèƒ½å¯¹è¯</div>
                <div class="feature-description">Qwen2.5 æ”¯æŒä¸Šä¸‹æ–‡è®°å¿†</div>
            </div>
            """)
            gr.HTML("""
            <div class="feature-card">
                <div class="feature-icon">ğŸ‘¤</div>
                <div class="feature-title">å£°çº¹è¯†åˆ«</div>
                <div class="feature-description">CAM++ ä¸“å±åŠ©æ‰‹éªŒè¯</div>
            </div>
            """)
            gr.HTML("""
            <div class="feature-card">
                <div class="feature-icon">ğŸ”Š</div>
                <div class="feature-title">è¯­éŸ³åˆæˆ</div>
                <div class="feature-description">Edge TTS å¤šè¯­ç§è‡ªç„¶è¯­éŸ³</div>
            </div>
            """)
        
        # é¡µè„š
        gr.Markdown("""
        ---
        <div style="text-align: center; color: #666; padding: 1rem;">
            ğŸ¦† éº»é¸­è¯­éŸ³åŠ©æ‰‹ v2.0 | Powered by ModelScope & Gradio | Made with â¤ï¸
        </div>
        """)
        
        # ========== äº‹ä»¶ç»‘å®š ==========
        
        # æ›´æ–°è®¾ç½®
        def update_settings(kws, word, sv, threshold, tts, prompt):
            return {
                "enable_kws": kws,
                "wake_word": word,
                "enable_sv": sv,
                "sv_threshold": threshold,
                "enable_tts": tts,
                "system_prompt": prompt
            }
        
        for component in [enable_kws, wake_word, enable_sv, sv_threshold, enable_tts, system_prompt]:
            component.change(
                fn=update_settings,
                inputs=[enable_kws, wake_word, enable_sv, sv_threshold, enable_tts, system_prompt],
                outputs=settings_state
            )
        
        # å‘é€æ¶ˆæ¯
        send_btn.click(
            fn=chat_respond,
            inputs=[msg, chatbot, audio_input, settings_state],
            outputs=[chatbot, audio_output]
        ).then(
            lambda: ("", None),
            outputs=[msg, audio_input]
        )
        
        msg.submit(
            fn=chat_respond,
            inputs=[msg, chatbot, audio_input, settings_state],
            outputs=[chatbot, audio_output]
        ).then(
            lambda: ("", None),
            outputs=[msg, audio_input]
        )
        
        # æ¸…ç©ºå¯¹è¯
        clear_btn.click(
            lambda: ([], None),
            outputs=[chatbot, audio_output]
        )
        
        # åŠ è½½æ¨¡å‹
        load_btn.click(
            fn=maya_models.load_models,
            outputs=load_status
        )
        
        # æ³¨å†Œå£°çº¹
        register_btn.click(
            fn=register_voiceprint,
            inputs=voiceprint_audio,
            outputs=register_status
        )
    
    return demo

# ========== ä¸»å‡½æ•° ==========

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="éº»é¸­è¯­éŸ³åŠ©æ‰‹ - ç°ä»£åŒ– Web UI")
    parser.add_argument("--port", type=int, default=7860, help="ç«¯å£å·")
    parser.add_argument("--share", action="store_true", help="ç”Ÿæˆå…¬ç½‘é“¾æ¥")
    parser.add_argument("--server-name", type=str, default="0.0.0.0", help="æœåŠ¡å™¨åœ°å€")
    args = parser.parse_args()
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘   ğŸ¦† éº»é¸­è¯­éŸ³åŠ©æ‰‹ v2.0 - ç°ä»£åŒ–ç•Œé¢       â•‘
    â•‘   Maya Voice Assistant - Modern UI        â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    ğŸ¨ ChatGPT/Claude é£æ ¼ç•Œé¢
    ğŸŒ“ æ”¯æŒæ·±è‰²/æµ…è‰²ä¸»é¢˜
    ğŸ’¬ æµå¼å¯¹è¯è¾“å‡º
    ğŸ“± å“åº”å¼å¸ƒå±€
    
    """)
    
    demo = create_modern_ui()
    
    # å¯ç”¨é˜Ÿåˆ—ä»¥æ”¯æŒæµå¼è¾“å‡ºå’Œè¿›åº¦æ¡
    demo.queue(concurrency_count=5, max_size=20)
    
    demo.launch(
        server_name=args.server_name,
        server_port=args.port,
        share=args.share,
        show_error=True,
        inbrowser=True  # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
    )

